{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BB84 QKD Parameters Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS for GPU acceleration.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if MPS is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use MPS as the device\n",
    "    print(\"Using MPS for GPU acceleration.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "    print(\"MPS is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp, factorial  # For basic math operations\n",
    "from scipy.optimize import minimize, dual_annealing, differential_evolution, Bounds  # For optional optimization methods\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import numpy as np  # For tensor operations if needed alongside PyTorch\n",
    "from tqdm import tqdm  # For progress bars\n",
    "from joblib import Parallel, delayed  # For parallel processing\n",
    "import os  # For file system operations\n",
    "import json  # For saving and loading datasets\n",
    "import time  # For timing operations\n",
    "import pandas as pd  # For data manipulation\n",
    "from tabulate import tabulate  # For pretty-printing tables\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the notebook's directory\n",
    "notebook_dir = os.getcwd()\n",
    "# Add parent directory to path\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "\n",
    "# PyTorch imports\n",
    "import torch  # Core PyTorch library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e_1\n",
    "# Fiber lengths\n",
    "Ls = torch.linspace(5, 200, 1000)  # Fiber lengths in km\n",
    "L_BC = Ls\n",
    "e_1 = L_BC / 100\n",
    "\n",
    "#e_2\n",
    "P_dc_value = torch.tensor(6e-7, device=device)   # Dark count probability\n",
    "Y_0 = P_dc_value\n",
    "# 2.7*10** -7\n",
    "# P_dc = 6 * 10 ** (-7)   # given in the paper, discussed with range from 10^-8 to 10^-5\n",
    "e_2 = -torch.log(Y_0)\n",
    "\n",
    "# e_3\n",
    "# Misalignment error probability\n",
    "# 4*1e-2          # given in the paper, discussed with range from 0 to 0.1\n",
    "e_mis = torch.tensor(5e-3, device=device)  # Misalignment error probability # given in the paper, discussed with range from 0 to 0.1 \n",
    "e_d = e_mis\n",
    "# 0.026 \n",
    "e_3 = e_d * 100\n",
    "\n",
    "# e_4\n",
    "# Detected events\n",
    "n_X_values = torch.tensor([10 ** s for s in range(4, 15)], dtype=torch.float64, device=device)  # Detected events # Detected events\n",
    "# n_X_values = torch.tensor([10**s for s in range(6, 11)], dtype=torch.int64)\n",
    "N = n_X_values\n",
    "e_4 = torch.log10(N)\n",
    "\n",
    "# Prepare input combinations\n",
    "# inputs = [(L, n_X) for L in torch.linspace(0.1, 200, 100) for n_X in n_X_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2  # Attenuation coefficient (dB/km), given in the paper\n",
    "eta_Bob = 0.1  # Detector efficiency, given in the paper\n",
    "P_ap = 0  # After-pulse probability\n",
    "f_EC = 1.16  # Error correction efficiency\n",
    "# secutity error \n",
    "epsilon_sec = 1e-10 # is equal to kappa * secrecy length Kl, range around 1e-10 Scalar, as it is a single value throughout the calculations.\n",
    "# correlation error\n",
    "epsilon_cor = 1e-15 # given in the paper, discussed with range from 0 to 10e-10\n",
    "# Dark count probability\n",
    "n_event = 1  # for single photon event\n",
    "# Misalignment error probability\n",
    "# 4*1e-2          # given in the paper, discussed with range from 0 to 0.1\n",
    "kappa = 1e-15           # given in the paper\n",
    "f_EC = 1.16             # given in the paper, range around 1.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_parameters(params):\n",
    "    mu_1, mu_2, P_mu_1, P_mu_2, P_X_value = params\n",
    "    mu_3 = 2e-4\n",
    "    P_mu_3 = 1 - P_mu_1 - P_mu_2\n",
    "    P_Z_value = 1 - P_X_value\n",
    "    mu_k_values = torch.tensor([mu_1, mu_2, mu_3])\n",
    "    return params, mu_3, P_mu_3, P_Z_value, mu_k_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QKD_Functions.QKD_Functions_Torch import (\n",
    "    calculate_factorial,\n",
    "    calculate_tau_n,\n",
    "    calculate_eta_ch,\n",
    "    calculate_eta_sys,\n",
    "    calculate_D_mu_k,\n",
    "    calculate_n_X_total,\n",
    "    calculate_N,\n",
    "    calculate_n_Z_total,\n",
    "    calculate_e_mu_k,\n",
    "    calculate_e_obs,\n",
    "    calculate_h,\n",
    "    calculate_lambda_EC,\n",
    "    calculate_sqrt_term,\n",
    "    calculate_tau_n,\n",
    "    calculate_n_pm, \n",
    "    calculate_S_0,\n",
    "    calculate_S_1,\n",
    "    calculate_m_mu_k,\n",
    "    calculate_m_pm,\n",
    "    calculate_v_1,\n",
    "    calculate_gamma,\n",
    "    calculate_Phi,\n",
    "    calculate_LastTwoTerm,\n",
    "    calculate_l,\n",
    "    calculate_R,\n",
    "    experimental_parameters,\n",
    "    other_parameters,\n",
    "    calculate_key_rates_and_metrics,\n",
    "    penalty, \n",
    "    objective,\n",
    "# objective_with_logging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounds for optimization parameters\n",
    "bounds = [\n",
    "    (1*1e-6, 1),  # mu_1, if it represents a mean photon number, adjust max as needed\n",
    "    (1*1e-6, 1),  # mu_2\n",
    "    (1*1e-6, 1),   # P_mu_1, probability\n",
    "    (1*1e-6, 1),   # P_mu_2, probability\n",
    "    (1*1e-6, 1),   # P_X_value, probability\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_single_instance(input_params, bounds, alpha, eta_Bob, P_dc_value, epsilon_sec, epsilon_cor, f_EC, e_mis, P_ap, n_event, max_retries=20):\n",
    "    \"\"\"\n",
    "    Optimize key rates using PyTorch optimization\n",
    "    \"\"\"\n",
    "    # Move inputs to device\n",
    "    L, n_X = input_params\n",
    "    L = torch.tensor(L, dtype=torch.float64, device=device)\n",
    "    n_X = torch.tensor(n_X, dtype=torch.float64, device=device)\n",
    "    \n",
    "    best_key_rate = float('-inf')\n",
    "    best_params = None\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Initialize parameters with random values within bounds\n",
    "            params = []\n",
    "            for (lower, upper) in bounds:\n",
    "                param = torch.rand(1, dtype=torch.float64, device=device) * (upper - lower) + lower\n",
    "                param.requires_grad_(True)\n",
    "                params.append(param)\n",
    "\n",
    "            # Use Adam optimizer\n",
    "            optimizer = torch.optim.Adam(params, lr=0.01)\n",
    "            \n",
    "            # Optimization loop\n",
    "            for i in range(500):  # max iterations\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Compute key rate\n",
    "                current_params = [p.item() for p in params]\n",
    "                key_rate = objective(current_params, L, n_X, alpha, eta_Bob, P_dc_value, \n",
    "                                  epsilon_sec, epsilon_cor, f_EC, e_mis, P_ap, n_event)[0]\n",
    "                \n",
    "                # Convert to tensor if not already\n",
    "                if not isinstance(key_rate, torch.Tensor):\n",
    "                    key_rate = torch.tensor(key_rate, dtype=torch.float64, device=device)\n",
    "                \n",
    "                # Negative for minimization\n",
    "                loss = -key_rate\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update parameters\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Project parameters back to bounds\n",
    "                with torch.no_grad():\n",
    "                    for param, (lower, upper) in zip(params, bounds):\n",
    "                        param.clamp_(lower, upper)\n",
    "                \n",
    "                # Check convergence\n",
    "                if i > 0 and abs(prev_loss - loss.item()) < 1e-6:\n",
    "                    break\n",
    "                prev_loss = loss.item()\n",
    "\n",
    "            # Get optimized parameters and key rate\n",
    "            optimized_params = [p.item() for p in params]\n",
    "            optimized_key_rate = -loss.item()\n",
    "\n",
    "            # Check if key rate is valid\n",
    "            if torch.log10(torch.tensor(max(optimized_key_rate, 1e-10))) > 0:\n",
    "                print(f\"Attempt {attempt + 1}: Abnormal key rate detected (log10 > 0). Retrying...\")\n",
    "                continue\n",
    "\n",
    "            # Update best results if improved\n",
    "            if optimized_key_rate > best_key_rate:\n",
    "                best_key_rate = optimized_key_rate\n",
    "                best_params = optimized_params\n",
    "\n",
    "            # Check if parameters have stabilized\n",
    "            if best_params is not None and torch.allclose(\n",
    "                torch.tensor(best_params, device=device),\n",
    "                torch.tensor(optimized_params, device=device),\n",
    "                atol=1e-4\n",
    "            ):\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1}: Optimization error: {e}. Retrying...\")\n",
    "\n",
    "    if best_params is not None:\n",
    "        return L.item(), n_X.item(), best_key_rate, best_params\n",
    "\n",
    "    print(f\"Optimization failed after {max_retries} retries.\")\n",
    "    return L.item(), n_X.item(), float('nan'), [float('nan')] * len(bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/0wg1hssn6n79tjc8mh6p_spr0000gn/T/ipykernel_3901/3589318213.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  n_X = torch.tensor(n_X, dtype=torch.float64, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 2: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 3: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 4: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 5: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 6: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 7: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 8: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 9: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 10: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 11: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 12: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 13: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 14: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 15: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 16: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 17: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 18: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 19: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Attempt 20: Optimization error: 'float' object has no attribute 'to'. Retrying...\n",
      "Optimization failed after 20 retries.\n",
      "Optimization for a single instance took 0.58 seconds.\n",
      "Fiber Length: 5.0 km, Detected Events (n_X): 10000.0\n",
      "Optimized Key Rate: nan\n",
      "Optimized Parameters:\n",
      "  mu_1: nan\n",
      "  mu_2: nan\n",
      "  P_mu_1: nan\n",
      "  P_mu_2: nan\n",
      "  P_X_value: nan\n"
     ]
    }
   ],
   "source": [
    "# Define input parameters for a single instance\n",
    "single_input = (Ls[0].item(), n_X_values[0])  # Example: first fiber length and first n_X value\n",
    "\n",
    "# Measure start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the optimization for the single instance\n",
    "L, n_X, optimized_key_rate, optimized_params = optimize_single_instance(  # Note the _torch suffix\n",
    "    single_input, bounds, alpha, eta_Bob, P_dc_value, epsilon_sec, epsilon_cor, f_EC, e_mis, P_ap, n_event\n",
    ")\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Output the results with parameter names\n",
    "parameter_names = [\"mu_1\", \"mu_2\", \"P_mu_1\", \"P_mu_2\", \"P_X_value\"]\n",
    "optimized_parameters = {name: value for name, value in zip(parameter_names, optimized_params)}\n",
    "\n",
    "print(f\"Optimization for a single instance took {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Fiber Length: {L} km, Detected Events (n_X): {n_X}\")\n",
    "print(f\"Optimized Key Rate: {optimized_key_rate:.3e}\")\n",
    "print(\"Optimized Parameters:\")\n",
    "for name, value in optimized_parameters.items():\n",
    "    print(f\"  {name}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "def generate_comprehensive_dataset(Ls, n_X_values, bounds, alpha, eta_Bob, P_dc_value, epsilon_sec, epsilon_cor, f_EC, e_mis, P_ap, n_event, n_jobs=12):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive dataset using parallel processing with PyTorch\n",
    "    \"\"\"\n",
    "    # Create all combinations of L and n_X\n",
    "    combinations = [(L.item(), float(n_X)) for L in Ls for n_X in n_X_values]\n",
    "    \n",
    "    # Initialize dictionary to store results\n",
    "    categorized_dataset = {float(n_X): [] for n_X in n_X_values}\n",
    "    \n",
    "    def process_single_combination(combo):\n",
    "        \"\"\"Process a single (L, n_X) combination\"\"\"\n",
    "        L, n_X_float = combo\n",
    "        \n",
    "        # Optimize parameters for this combination\n",
    "        result = optimize_single_instance(\n",
    "            (L, n_X_float), bounds, alpha, eta_Bob, P_dc_value, \n",
    "            epsilon_sec, epsilon_cor, f_EC, e_mis, P_ap, n_event\n",
    "        )\n",
    "        \n",
    "        if result is None or result[2] <= 0:  # Skip if optimization failed or key rate is invalid\n",
    "            return None\n",
    "            \n",
    "        L_val, _, penalized_key_rate, optimized_params = result\n",
    "        mu_1, mu_2, P_mu_1, P_mu_2, P_X_value = optimized_params\n",
    "        \n",
    "        # Compute normalized parameters using PyTorch\n",
    "        return {\n",
    "            \"n_X\": n_X_float,\n",
    "            \"fiber_length\": float(L_val),\n",
    "            \"e_1\": float(L_val / 100),\n",
    "            \"e_2\": float(-torch.log10(torch.tensor(P_dc_value, device=device))),\n",
    "            \"e_3\": float(e_mis * 100),\n",
    "            \"e_4\": float(torch.log10(torch.tensor(n_X_float, device=device))),\n",
    "            \"key_rate\": float(max(penalized_key_rate, 1e-10)),\n",
    "            \"optimized_params\": {\n",
    "                \"mu_1\": float(mu_1),\n",
    "                \"mu_2\": float(mu_2),\n",
    "                \"P_mu_1\": float(P_mu_1),\n",
    "                \"P_mu_2\": float(P_mu_2),\n",
    "                \"P_X_value\": float(P_X_value)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # ... rest of the function remains the same ...\n",
    "\n",
    "# Usage with PyTorch\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define parameter space with PyTorch\n",
    "Ls = torch.linspace(5, 200, 1000, device=device)  # 1000 points from 5 to 200 km\n",
    "n_X_values = [10**s for s in range(4, 15)]  # n_X from 10^4 to 10^14\n",
    "\n",
    "# Generate the comprehensive dataset with parallel processing\n",
    "dataset = generate_comprehensive_dataset(\n",
    "    Ls, n_X_values, bounds, alpha, eta_Bob, P_dc_value, \n",
    "    epsilon_sec, epsilon_cor, f_EC, e_mis, P_ap, n_event,\n",
    "    n_jobs=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasonableness Check for Execution Time:\n",
    "\n",
    "A single optimization took 1.81 seconds. For 10,000 instances, the time would scale proportionally if no optimizations (e.g., batch parallelism) are applied:\n",
    "\n",
    "$ \\text{Total Time} = 1.81 \\times 10,000 \\approx 5.03 \\, \\text{hours}$ \n",
    "\n",
    "With 12 CPU cores using joblib, the time should reduce by approximately  1 / 12 :\n",
    "\n",
    "$ \\text{Total Time with 12 CPUs} \\approx 5.03 / 12 \\approx 25.13 \\, \\text{minutes}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Dataset Generation Using joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(Ls, n_X_values, bounds, alpha, eta_Bob, P_dc_value, epsilon_sec, epsilon_cor, f_EC, e_mis, P_ap, n_event, device=None):\n",
    "    \"\"\"\n",
    "    Generate a dataset by optimizing key rates for various fiber lengths and n_X values.\n",
    "    \"\"\"\n",
    "    # Create input combinations\n",
    "    inputs = [(L.item(), n_X.item()) for L in Ls for n_X in n_X_values]\n",
    "\n",
    "    print(\"Generating dataset...\")\n",
    "    results = []\n",
    "\n",
    "    def process_batch(batch):\n",
    "        batch_results = []\n",
    "        for input_params in batch:\n",
    "            L, n_X, penalized_key_rate, optimized_params = optimize_single_instance_on_gpu(\n",
    "                input_params, bounds, alpha, eta_Bob, P_dc_value, epsilon_sec, epsilon_cor, f_EC, e_mis, P_ap, n_event\n",
    "            )\n",
    "            batch_results.append((L, n_X, penalized_key_rate, optimized_params))\n",
    "        return batch_results\n",
    "\n",
    "    # Split inputs into batches\n",
    "    batch_size = 100\n",
    "    batches = [inputs[i:i+batch_size] for i in range(0, len(inputs), batch_size)]\n",
    "\n",
    "\n",
    "    # Progress bar for monitoring\n",
    "    with tqdm(total=len(batches), desc=\"Generating Dataset\") as progress_bar:\n",
    "        results = Parallel(n_jobs=12)(\n",
    "            delayed(process_batch)(batch) for batch in batches\n",
    "        )\n",
    "        progress_bar.update(len(batches))\n",
    "\n",
    "    # Flatten results\n",
    "    results = [item for sublist in results for item in sublist]\n",
    "\n",
    "    # Process results into a dataset\n",
    "    dataset = []\n",
    "    for L, n_X, penalized_key_rate, optimized_params in results:\n",
    "        # Extract optimized parameters (ensure the order matches the bounds setup)\n",
    "        mu_1, mu_2, P_mu_1, P_mu_2, P_X_value = optimized_params\n",
    "\n",
    "        # Compute normalized parameters\n",
    "        e_1 = L / 100  # Normalize fiber length\n",
    "        e_2 = -torch.log10(P_dc_value).item()  # Normalize dark count probability\n",
    "        e_3 = (e_mis * 100).item()  # Normalize misalignment error probability\n",
    "        e_4 = torch.log10(torch.tensor(n_X, dtype=torch.float64, device=device)).item()  # Normalize number of pulses\n",
    "\n",
    "        # Append processed data\n",
    "        dataset.append({\n",
    "            \"e_1\": e_1,\n",
    "            \"e_2\": e_2,\n",
    "            \"e_3\": e_3,\n",
    "            \"e_4\": e_4,\n",
    "            \"key_rate\": penalized_key_rate,\n",
    "            \"optimized_params\": {\n",
    "                \"mu_1\": mu_1,\n",
    "                \"mu_2\": mu_2,\n",
    "                \"P_mu_1\": P_mu_1,\n",
    "                \"P_mu_2\": P_mu_2,\n",
    "                \"P_X_value\": P_X_value,\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Experiment-specific parameters\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Define parameters as PyTorch tensors\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m P_dc_value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m6e-7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Dark count probability\u001b[39;00m\n\u001b[1;32m      5\u001b[0m e_mis \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m5e-3\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64)       \u001b[38;5;66;03m# Misalignment error probability\u001b[39;00m\n\u001b[1;32m      6\u001b[0m alpha \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.2\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64)        \u001b[38;5;66;03m# Attenuation coefficient (dB/km), given in the paper\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "# Experiment-specific parameters\n",
    "\n",
    "# Define parameters as PyTorch tensors\n",
    "P_dc_value = torch.tensor(6e-7, device=device, dtype=torch.float64)  # Dark count probability\n",
    "e_mis = torch.tensor(5e-3, device=device, dtype=torch.float64)       # Misalignment error probability\n",
    "alpha = torch.tensor(0.2, device=device, dtype=torch.float64)        # Attenuation coefficient (dB/km), given in the paper\n",
    "eta_Bob = torch.tensor(0.1, device=device, dtype=torch.float64)      # Detector efficiency, given in the paper\n",
    "P_ap = torch.tensor(1e-6, device=device, dtype=torch.float64)        # After-pulse probability # 4*1e-2          # given in the paper, discussed with range from 0 to 0.1\n",
    "f_EC = torch.tensor(1.16, device=device, dtype=torch.float64)        # Error correction efficiency # given in the paper, range around 1.1\n",
    "epsilon_sec = torch.tensor(1e-10, device=device, dtype=torch.float64) # Security error # is equal to kappa * secrecy length Kl, range around 1e-10 Scalar, as it is a single value throughout the calculations.\n",
    "epsilon_cor = torch.tensor(1e-15, device=device, dtype=torch.float64) # Correlation error\n",
    "n_event = torch.tensor(1, device=device, dtype=torch.float64)         # For single photon event\n",
    "kappa = torch.tensor(1e-15, device=device, dtype=torch.float64)       # given in the papere_1 # given in the paper, discussed with range from 0 to 10e-10\n",
    "\n",
    "# Define parameter space\n",
    "Ls = torch.linspace(0.1, 220, 100, device=device, dtype=torch.float64)  # Fiber lengths\n",
    "n_X_values = torch.logspace(6, 10, 8, device=device, dtype=torch.float64)  # Detected events\n",
    "\n",
    "\n",
    "# \n",
    "# torch.linspace(0.1, 220, 10):\n",
    "# \t•\tGenerates 10 equally spaced values between 0.1 and 220. For example: [0.1, 27.88, 55.66, ..., 220].\n",
    "# \t•\tRepresents 10 fiber lengths to evaluate, rather than 1000.\n",
    "# np.logspace(6, 8, 5):\n",
    "# \t•\tGenerates 5 logarithmically spaced values between 10^6 and 10^8. For example: [1e6, 3.16e6, 1e7, ..., 1e8].\n",
    "# \t•\tRepresents the number of detected events (n_X) in a smaller range with fewer steps.\n",
    "\n",
    "import cProfile\n",
    "cProfile.run(\"\"\"\n",
    "optimize_single_instance(\n",
    "    (Ls[0].item(), n_X_values[0].item()), bounds, alpha, eta_Bob, P_dc_value, epsilon_sec, epsilon_cor, f_EC, e_mis, P_ap, n_event, device=device\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Measure total dataset generation time\n",
    "start_time = time.time()\n",
    "\n",
    "dataset = generate_dataset(\n",
    "    Ls, n_X_values, bounds, alpha, eta_Bob, P_dc_value, epsilon_sec, epsilon_cor, f_EC, e_mis, P_ap, n_event, device=device\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Dataset generation completed in {(end_time - start_time) / 60:.2f} minutes.\")\n",
    "\n",
    "# Save to JSON\n",
    "output_filename = \"training_dataset.json\"\n",
    "with open(output_filename, \"w\") as f:\n",
    "    json.dump(dataset, f)\n",
    "print(f\"Dataset saved as '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "with open(\"training_dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract fiber lengths and key rates\n",
    "e_1 = torch.tensor([item[\"e_1\"] * 100 for item in data], dtype=torch.float64)  # Denormalize fiber lengths (convert to km)\n",
    "key_rate = torch.tensor([item[\"key_rate\"] for item in data], dtype=torch.float64)  # Correct key name\n",
    "\n",
    "# Extract optimized parameters\n",
    "mu_1 = torch.tensor([item[\"optimized_params\"][\"mu_1\"] for item in data], dtype=torch.float64)  # Access nested keys\n",
    "mu_2 = torch.tensor([item[\"optimized_params\"][\"mu_2\"] for item in data], dtype=torch.float64)\n",
    "P_mu_1 = torch.tensor([item[\"optimized_params\"][\"P_mu_1\"] for item in data], dtype=torch.float64)\n",
    "P_mu_2 = torch.tensor([item[\"optimized_params\"][\"P_mu_2\"] for item in data], dtype=torch.float64)\n",
    "P_X_value = torch.tensor([item[\"optimized_params\"][\"P_X_value\"] for item in data], dtype=torch.float64)\n",
    "\n",
    "# Sort by fiber length for smooth plotting\n",
    "sorted_indices = torch.argsort(e_1)\n",
    "e_1_sorted = e_1[sorted_indices]\n",
    "key_rate_sorted = key_rate[sorted_indices]\n",
    "mu_1_sorted = mu_1[sorted_indices]\n",
    "mu_2_sorted = mu_2[sorted_indices]\n",
    "P_mu_1_sorted = P_mu_1[sorted_indices]\n",
    "P_mu_2_sorted = P_mu_2[sorted_indices]\n",
    "P_X_value_sorted = P_X_value[sorted_indices]\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Left plot: Penalized Key Rate\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(e_1_sorted.cpu(), torch.log10(torch.clamp(key_rate_sorted, min=1e-10)).cpu(), label=\"Penalized Key Rate (log10)\")\n",
    "plt.xlabel(\"Fiber Length (km)\")\n",
    "plt.ylabel(\"log10(Penalized Key Rate)\")\n",
    "plt.title(\"Penalized Key Rate vs Fiber Length\")\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "\n",
    "# Right plot: Optimized Parameters\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(e_1_sorted.cpu(), mu_1_sorted.cpu(), label=\"mu_1\")\n",
    "plt.plot(e_1_sorted.cpu(), mu_2_sorted.cpu(), label=\"mu_2\")\n",
    "plt.plot(e_1_sorted.cpu(), P_mu_1_sorted.cpu(), label=\"P_mu_1\")\n",
    "plt.plot(e_1_sorted.cpu(), P_mu_2_sorted.cpu(), label=\"P_mu_2\")\n",
    "plt.plot(e_1_sorted.cpu(), P_X_value_sorted.cpu(), label=\"P_X_value\")\n",
    "plt.xlabel(\"Fiber Length (km)\")\n",
    "plt.ylabel(\"Optimized Parameters\")\n",
    "plt.title(\"Optimized Parameters vs Fiber Length\")\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[0])  # Print the first item to inspect its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "with open(\"training_dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the top 100 entries\n",
    "top_100_entries = data[:100]\n",
    "\n",
    "# Display the top 100 entries\n",
    "print(\"\\nTop 100 entries:\")\n",
    "for idx, entry in enumerate(top_100_entries, 1):\n",
    "    print(f\"Entry {idx}: {entry}\")\n",
    "\n",
    "# Optional: Flatten the JSON structure and save as a CSV file\n",
    "df = pd.json_normalize(data, sep='_')  # Flatten the JSON structure\n",
    "\n",
    "# Convert numeric data to PyTorch tensors where applicable\n",
    "for col in df.select_dtypes(include=['float', 'int']).columns:\n",
    "    df[col] = df[col].apply(lambda x: torch.tensor(x, dtype=torch.float64))\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "output_csv_file = \"training_dataset.csv\"\n",
    "df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "print(f\"\\nCSV file saved as '{output_csv_file}'.\")\n",
    "\n",
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the JSON data\n",
    "# with open(\"training_dataset.json\", \"r\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# # Flatten the JSON structure\n",
    "# df = pd.json_normalize(data, sep='_')\n",
    "\n",
    "# # Save the DataFrame as a CSV file\n",
    "# output_csv_file = \"training_dataset.csv\"\n",
    "# df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "# print(f\"CSV file saved as {output_csv_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounds = Bounds([1e-6] * 5, [1.0] * 5)  # Example bounds\n",
    "# input_params = (100, 1e8)  # Example fiber length and n_X\n",
    "# result = optimize_single_instance(\n",
    "#     input_params, bounds, alpha, eta_Bob, P_dc_value, epsilon_sec, epsilon_cor, f_EC, e_mis, P_ap, n_event\n",
    "# )\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Setup\n",
    "\n",
    "49% of 10,000 iterations completed in 15 minutes. \n",
    "\n",
    "Processing speed: 5.16 iterations per second (it/s). \n",
    "\n",
    "Total Time Estimation (Current Setup): \\\n",
    "Total iterations: 10,000. \\\n",
    "Completed iterations:  10,000 \\times 0.49 = 4,900 . \\\n",
    "Time to complete 4,900 iterations: 15 minutes (900 seconds). \\\n",
    "Estimated total time for 10,000 iterations: \n",
    "\n",
    "$\\text{Total time} = \\frac{\\text{Total iterations}}{\\text{Processing speed}} = \\frac{10,000}{5.16} \\approx 1,937 \\text{ seconds (32 minutes)}$\n",
    "\n",
    "So, approximately 32 minutes total is needed for the dataset generation with your current setup.\n",
    "\n",
    "## Multiprocessing\n",
    "\n",
    "Assumption:\n",
    "12 CPU cores available (based on earlier discussions). \\\n",
    "Multiprocessing scales linearly with cores (ideal case, no overhead). \n",
    "\n",
    "Parallel Speed Calculation:\n",
    "\n",
    "If multiprocessing scales ideally: \n",
    "\n",
    "$\\text{Parallel speed} = \\text{Single-threaded speed} \\times \\text{Number of cores}$\n",
    "\n",
    "\n",
    "$\\text{Parallel speed} = 5.16 \\, \\text{it/s} \\times 12 \\approx 61.92 \\, \\text{it/s}$\n",
    "\n",
    "\n",
    "Parallel Time Calculation:\n",
    "\n",
    "\n",
    "$\\text{Total time (parallel)} = \\frac{\\text{Total iterations}}{\\text{Parallel speed}} = \\frac{10,000}{61.92} \\approx 161.5 \\, \\text{seconds (2.7 minutes)}.$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. https://machinelearningmastery.com/dual-annealing-optimization-with-python/\n",
    "2. https://en.wikipedia.org/wiki/Global_optimization\n",
    "3. https://docs.scipy.org/doc/scipy/tutorial/optimize.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
